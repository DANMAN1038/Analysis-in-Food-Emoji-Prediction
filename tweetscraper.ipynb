{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYHLn4CvMyVF73Nmt8BxBI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"i3ZjGQeoxIAC","outputId":"180f8c98-ad68-4975-fa80-b75fa2908421","executionInfo":{"status":"error","timestamp":1682001146857,"user_tz":300,"elapsed":28942,"user":{"displayName":"Danial Syed","userId":"05182636100941851945"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n","  Cloning https://github.com/JustAnotherArchivist/snscrape.git to /tmp/pip-req-build-oyep_lx1\n","  Running command git clone --filter=blob:none --quiet https://github.com/JustAnotherArchivist/snscrape.git /tmp/pip-req-build-oyep_lx1\n","  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit 3dd9c28e31b8babeb2a187fbae994d9717ded168\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from snscrape==0.6.2.20230321.dev3+g3dd9c28) (2.27.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from snscrape==0.6.2.20230321.dev3+g3dd9c28) (3.11.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from snscrape==0.6.2.20230321.dev3+g3dd9c28) (4.9.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from snscrape==0.6.2.20230321.dev3+g3dd9c28) (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->snscrape==0.6.2.20230321.dev3+g3dd9c28) (2.4.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape==0.6.2.20230321.dev3+g3dd9c28) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape==0.6.2.20230321.dev3+g3dd9c28) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape==0.6.2.20230321.dev3+g3dd9c28) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape==0.6.2.20230321.dev3+g3dd9c28) (3.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape==0.6.2.20230321.dev3+g3dd9c28) (1.7.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:snscrape.base:Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=food+since%3A2021-07-05+until%3A2022-07-06&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: non-200 status code (401)\n","CRITICAL:snscrape.base:4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=food+since%3A2021-07-05+until%3A2022-07-06&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n","CRITICAL:snscrape.base:Errors: non-200 status code (401), non-200 status code (401), non-200 status code (401), non-200 status code (401)\n"]},{"output_type":"error","ename":"ScraperException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-1e4d6e8982d8>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mattributes_container\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msntwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTwitterSearchScraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'food since:2021-07-05 until:2022-07-06'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36mget_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cursor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://api.twitter.com/2/search/adaptive.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_TwitterAPIType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaginationParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2_timeline_instructions_to_tweets_or_users\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_iter_api_data\u001b[0;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Retrieving scroll page {cursor}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapiType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreqParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_get_api_data\u001b[0;34m(self, endpoint, apiType, params)\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mapiType\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_TwitterAPIType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAPHQL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m                         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquote_via\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apiHeaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponseOkCallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_api_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m                         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/snscrape/base.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/snscrape/base.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    245\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Errors: {\", \".join(errors)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mScraperException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reached unreachable code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mScraperException\u001b[0m: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=food+since%3A2021-07-05+until%3A2022-07-06&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up."]}],"source":["# https://www.freecodecamp.org/news/python-web-scraping-tutorial/\n","# Run the pip install command below if you don't already have the library\n","!pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n","\n","# Run the below command if you don't already have Pandas\n","!pip install pandas\n","import snscrape.modules.twitter as sntwitter\n","import pandas as pd\n","\n","# Creating list to append tweet data to\n","attributes_container = []\n","\n","for i,tweet in enumerate(sntwitter.TwitterSearchScraper('food since:2021-07-05 until:2022-07-06').get_items()):\n","    if i>20:\n","        break\n","    attributes_container.append([tweet.content])\n","print(attributes_container)\n","\n","# Using TwitterSearchScraper to scrape data and append tweets to list\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('acidic food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.content])\n","\n","# print(attributes_container)\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('acidic meal since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('bitter food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('bitter snack since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('citrusy food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('citrusy snack since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('cold food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('cold snack since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('cold meal since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('earthy food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('earthy snack since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('fresh food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('fresh meal since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('fruity snack since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('nutty food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('rich meal since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('robust food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","  \n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('sharp flavor since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('sour food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('sour snack since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('spicy food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('spicy meal since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('sweet food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('sweet snack since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('tangy snack since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","\n","# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('zesty food since:2021-07-05 until:2022-07-06').get_items()):\n","#     if i>20:\n","#         break\n","#     attributes_container.append([tweet.user.username, tweet.date, tweet.content])\n","    \n","# Creating a dataframe to load the list\n","tweets_df = pd.DataFrame(attributes_container, columns = [\"Tweet\"])\n","tweets_df.head(1000)"]}]}